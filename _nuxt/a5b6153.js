(window.webpackJsonp=window.webpackJsonp||[]).push([[5],{450:function(e,t,n){"use strict";n(11),n(6),n(12),n(69),n(35),n(297),n(207),n(70),n(89);var o=n(0);var r,c=n(71);t.a=(r="container",o.a.extend({name:"v-".concat(r),functional:!0,props:{id:String,tag:{type:String,default:"div"}},render:function(e,t){var n=t.props,data=t.data,o=t.children;data.staticClass="".concat(r," ").concat(data.staticClass||"").trim();var c=data.attrs;if(c){data.attrs={};var l=Object.keys(c).filter((function(e){if("slot"===e)return!1;var t=c[e];return e.startsWith("data-")?(data.attrs[e]=t,!1):t||"string"==typeof t}));l.length&&(data.staticClass+=" ".concat(l.join(" ")))}return n.id&&(data.domProps=data.domProps||{},data.domProps.id=n.id),e(n.tag,data,o)}})).extend({name:"v-container",functional:!0,props:{id:String,tag:{type:String,default:"div"},fluid:{type:Boolean,default:!1}},render:function(e,t){var n,o=t.props,data=t.data,r=t.children,l=data.attrs;return l&&(data.attrs={},n=Object.keys(l).filter((function(e){if("slot"===e)return!1;var t=l[e];return e.startsWith("data-")?(data.attrs[e]=t,!1):t||"string"==typeof t}))),o.id&&(data.domProps=data.domProps||{},data.domProps.id=o.id),e(o.tag,Object(c.a)(data,{staticClass:"container",class:Array({"container--fluid":o.fluid}).concat(n||[])}),r)}})},502:function(e,t,n){"use strict";n.r(t);var o={name:"ChallengeTaskSection",data:function(){return{}}},r=n(60),c=n(77),l=n.n(c),d=n(444),h=n(450),m=n(446),component=Object(r.a)(o,(function(){var e=this,t=e.$createElement,n=e._self._c||t;return n("section",{staticClass:"gnt-sec-2"},[n("section",{staticClass:"gnt-sec-3"},[n("v-container",[n("v-row",[n("v-col",{attrs:{cols:"12"}},[n("h2",[e._v("Challnge Overview")])])],1),e._v(" "),n("v-row",[n("v-col",{attrs:{cols:"12"}},[n("h3",[e._v("Background")]),e._v(" "),n("p",[e._v("\n            In order to improve the efficiency of manual labor, apart from\n            daily life settings, Human Activity Recognition has been applied\n            in industrial domains as well. Work processes, such as production\n            line systems inside factories or packaging tasks at logistics\n            centers, still depend mainly on human workers. To deal with the\n            rapidly changing demands of customers and suppliers, tasks by\n            human workers are expected to continue to play an important role\n            in the future. Therefore, quantifying such manual work is crucial\n            for streamlining the existing processes, finding bottlenecks,\n            assessing a worker’s performance, and detecting outliers.\n          ")]),e._v(" "),n("p",[e._v("\n            In many of these manual jobs, such as performing packaging tasks\n            in the logistic domain, human workers repetitively perform a\n            typical series of sequential operations, and with each iteration\n            (i.e., work period) comprising a sequence of operations such as\n            assembling a shipping box and filling up the box with items.\n            Getting meaningful information about each operation such as its\n            temporal location, average duration, and its abnormalities is\n            crucial for optimizing the work process. However, because the\n            varying size and number of items to pack, and the size of shipping\n            boxes depend on individual shipping orders, sensor data collected\n            in different work periods, as well as the duration of the same\n            operation in different work periods, can vary. These\n            characteristics of packaging work make its recognition a\n            challenging task.\n          ")]),e._v(" "),n("p",[e._v("\n            In this competition, you'll develop a model to recognize the 10\n            work operations in the packaging work. If successful, your work\n            will help the ubiquitous research community improve current smart\n            factories and better integrate human factors into the smart\n            factory optimization process.\n          ")])])],1),e._v(" "),n("v-row",[n("v-col",{attrs:{cols:"12"}},[n("h3",[e._v("Task")]),e._v(" "),n("p",[e._v("\n            In this competition, you’ll develop a model to recognize the\n            operations that conform packaging work from 4 IMU streams,\n            keypoint data, etc. The packaging work consists of 10 operations\n            (i.e., activity classes) described below. To quantify the\n            operations as precisely as possible, dense labeling is required.\n            You must predict activity classes for each 1 second-long time\n            slot. You can use data from TBA subjects to develop your model.\n            The test data is TBA sessions from TBA, TBA periods in total.\n          ")])])],1),e._v(" "),n("v-row",[n("v-col",{attrs:{cols:"12"}},[n("h3",[e._v("Evaluation")]),e._v(" "),n("p",[e._v("\n            The problem to be solved is the multiclass classification problem\n            of time-series data. You must predict the operation class for each\n            time slot. The time slot is set to 1 second in this challenge.\n          ")]),e._v(" "),n("p",[e._v('\n            The F1 measure (macro-average) is used as the evaluation metric.\n            F1-measure is calculated for each class and the average of them is\n            used as the score. Segments corresponding to the "Null" class are\n            excluded before evaluation. The winner will be selected based on\n            this metric evaluated on the test data.\n          ')]),e._v(" "),n("p",[e._v("\n            Data to be evaluated are the data within the session described in\n            the\n            "),n("a",{attrs:{href:"https://github.com/open-pack/openpack-toolkit/blob/main/docs/USER.md",target:"_blank"}},[e._v("USER.md")]),e._v(", i.e., the range of [Stat, End) (Note: the End time is not\n            included). The time slot starts from the Session Start time. For\n            example, on U0102-S0500, Start and End timestamp is Start =\n            2021-10-22 15:56:26+09:00 (unixtime=1634885786000) , End =\n            2021-10-22 16:26:48+09:00 (unixtime = 1634887608000) respectively.\n            Therefore, the timeslot to be evaluated is [1634885786000 (=\n            Start), 1634885787000, 1634885788000, ..., 1634887607000 (= End -\n            1s)].\n          ")]),e._v(" "),n("p",[e._v("\n            Script to calculate your score is available here ("),n("a",{attrs:{href:"https://github.com/open-pack/openpack-toolkit-dev/blob/main/openpack_toolkit/codalab/operation_segmentation/eval.py",target:"_blank"}},[e._v("eval_operation_segmentation()")]),e._v(")!\n          ")])])],1),e._v(" "),n("v-row",[n("v-col",{attrs:{cols:"12"}},[n("h3",[e._v("Data")])]),e._v(" "),n("v-col",{attrs:{cols:"12"}},[e._v(" TBA ")])],1),e._v(" "),n("v-row",[n("v-col",{attrs:{cols:"12"}},[n("h3",[e._v("Prize for Best 3 Teams")]),e._v(" "),n("ul",[n("li",[e._v("\n              Cash prize at the award ceremony @\n              "),n("a",{attrs:{href:"https://bio-navigation.jp/bird2023/",target:"_blank"}},[e._v("BiRD2023")])]),e._v(" "),n("li",[e._v("\n              Travel fee support (1 member from each team, up to\n              500,000JPY/person)\n            ")])]),e._v(" "),n("p",{staticClass:"mt-3 op-brown--text font-weight-bold"},[e._v("\n            To get these prize, each winner team should submit a technical\n            report by 1/31.\n          ")])])],1)],1)],1)])}),[],!1,null,"399b149f",null);t.default=component.exports;l()(component,{VCol:d.a,VContainer:h.a,VRow:m.a})}}]);